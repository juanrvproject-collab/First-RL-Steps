{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d72571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Configuraciones\n",
    "CELL_SIZE = 20  \n",
    "MAZE_SIZE = 35  \n",
    "WIDTH = MAZE_SIZE * CELL_SIZE\n",
    "HEIGHT = MAZE_SIZE * CELL_SIZE\n",
    "FPS = 60\n",
    "\n",
    "# Colores\n",
    "COLOR_WALL = (20, 80, 20)       \n",
    "COLOR_PATH = (230, 220, 200)    \n",
    "COLOR_AGENT = (255, 50, 50)     \n",
    "COLOR_GOAL = (255, 215, 0)      \n",
    "COLOR_GOAL_LOCKED = (100, 100, 50) \n",
    "COLOR_CHECKPOINT = (0, 100, 255) \n",
    "COLOR_CHECKED = (150, 150, 150)  \n",
    "\n",
    "class HedgeMazeEnv:\n",
    "    def __init__(self, size=35):\n",
    "        self.size = size\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        \n",
    "        self.maze = np.ones((self.size, self.size), dtype=int)\n",
    "        self._generate_maze(1, 1)\n",
    "        self._add_multiple_paths(40) \n",
    "        \n",
    "        self.goal_pos = [self.size - 2, self.size - 2]\n",
    "        self.maze[self.goal_pos[0], self.goal_pos[1]] = 0\n",
    "\n",
    "        s = self.size\n",
    "        potential_cp = [(1, s-2), (s-2, 1), (s//2, s//2), (s//4, s-s//4)]\n",
    "        self.checkpoints = []\n",
    "        for r, c in potential_cp:\n",
    "            if self.maze[r, c] == 1:\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if self.maze[r+dr, c+dc] == 0:\n",
    "                            r, c = r+dr, c+dc\n",
    "                            break\n",
    "            self.checkpoints.append([r, c])\n",
    "\n",
    "        self.n_states = (self.size, self.size, 16) \n",
    "        self.n_actions = 4  \n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def _generate_maze(self, x, y):\n",
    "        self.maze[x, y] = 0\n",
    "        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "        random.shuffle(directions)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 < nx < self.size-1 and 0 < ny < self.size-1 and self.maze[nx, ny] == 1:\n",
    "                self.maze[x + dx//2, y + dy//2] = 0\n",
    "                self._generate_maze(nx, ny)\n",
    "\n",
    "    def _add_multiple_paths(self, num_walls_to_remove):\n",
    "        count = 0\n",
    "        while count < num_walls_to_remove:\n",
    "            r = random.randint(1, self.size - 2)\n",
    "            c = random.randint(1, self.size - 2)\n",
    "            if self.maze[r, c] == 1:\n",
    "                self.maze[r, c] = 0\n",
    "                count += 1\n",
    "\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n_actions - 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = [1, 1]\n",
    "        self.visited_checkpoints = [False] * len(self.checkpoints)\n",
    "        self.steps = 0\n",
    "        self.max_steps = 1000 \n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        cp_id = sum([1 << i for i, visited in enumerate(self.visited_checkpoints) if visited])\n",
    "        return (self.agent_pos[0], self.agent_pos[1], cp_id)\n",
    "\n",
    "    def step(self, action):\n",
    "        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)] \n",
    "        dr, dc = moves[action]\n",
    "        new_pos = [self.agent_pos[0] + dr, self.agent_pos[1] + dc]\n",
    "\n",
    "        reward = -0.01 \n",
    "        done = False\n",
    "\n",
    "        if self.maze[new_pos[0], new_pos[1]] == 0:\n",
    "            self.agent_pos = new_pos\n",
    "            for i, cp in enumerate(self.checkpoints):\n",
    "                if self.agent_pos == cp and not self.visited_checkpoints[i]:\n",
    "                    self.visited_checkpoints[i] = True\n",
    "                    reward = 5.0  # Subimos el premio por checkpoint \n",
    "        else:\n",
    "            reward = -0.05 \n",
    "\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            if all(self.visited_checkpoints):\n",
    "                reward = 50.0 #  premio final\n",
    "                done = True\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        return self._get_obs(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "            pygame.display.set_caption(\"Bush Maze RL - All Checkpoints\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        pygame.event.pump() \n",
    "        self.screen.fill(COLOR_PATH)\n",
    "\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                rect = pygame.Rect(c * CELL_SIZE, r * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "                if self.maze[r, c] == 1:\n",
    "                    pygame.draw.rect(self.screen, COLOR_WALL, rect)\n",
    "                \n",
    "        for i, cp in enumerate(self.checkpoints):\n",
    "            color = COLOR_CHECKED if self.visited_checkpoints[i] else COLOR_CHECKPOINT\n",
    "            cp_rect = pygame.Rect(cp[1] * CELL_SIZE + 4, cp[0] * CELL_SIZE + 4, CELL_SIZE - 8, CELL_SIZE - 8)\n",
    "            pygame.draw.rect(self.screen, color, cp_rect, border_radius=3)\n",
    "\n",
    "        goal_rect = pygame.Rect(self.goal_pos[1] * CELL_SIZE, self.goal_pos[0] * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "        current_goal_color = COLOR_GOAL if all(self.visited_checkpoints) else COLOR_GOAL_LOCKED\n",
    "        pygame.draw.ellipse(self.screen, current_goal_color, goal_rect)\n",
    "\n",
    "        agent_rect = pygame.Rect(self.agent_pos[1] * CELL_SIZE, self.agent_pos[0] * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "        pygame.draw.rect(self.screen, COLOR_AGENT, agent_rect, border_radius=5)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfab75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 500/5000 completado.\n",
      "Episodio 1000/5000 completado.\n",
      "Episodio 1500/5000 completado.\n",
      "Episodio 2000/5000 completado.\n",
      "Episodio 2500/5000 completado.\n",
      "Episodio 3000/5000 completado.\n",
      "Episodio 3500/5000 completado.\n",
      "Episodio 4000/5000 completado.\n",
      "Episodio 4500/5000 completado.\n",
      "Episodio 5000/5000 completado.\n",
      "Entrenamiento finalizado.\n"
     ]
    }
   ],
   "source": [
    "env = HedgeMazeEnv()\n",
    "state_space = env.n_states\n",
    "action_space = env.n_actions\n",
    "    \n",
    "# CAMBIO 5: Más episodios y decaimiento más lento porque el problema es mucho más difícil\n",
    "numeroepisodios = 5000  \n",
    "learning_rate = 0.7          \n",
    "max_steps = 1000              \n",
    "gamma = 0.95                 \n",
    "max_epsilon = 1.0            \n",
    "min_epsilon = 0.05           \n",
    "decay_rate = 0.001 # Baja más despacio para explorar más\n",
    "\n",
    "Qtable = np.zeros((*state_space, action_space))\n",
    "\n",
    "for episodio in range(numeroepisodios):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episodio) \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            max_q = np.max(Qtable[state])\n",
    "            best_actions = [a for a in range(action_space) if Qtable[state][a] == max_q]\n",
    "            action = random.choice(best_actions)\n",
    "        else:\n",
    "            action = env.sample()\n",
    "        \n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        Qtable[state + (action,)] = Qtable[state + (action,)] + learning_rate * (\n",
    "            reward + gamma * np.max(Qtable[new_state]) - Qtable[state + (action,)]\n",
    "        )\n",
    "\n",
    "        if episodio >= numeroepisodios - 5:\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "    if (episodio + 1) % 500 == 0:\n",
    "        print(f\"Episodio {episodio + 1}/{numeroepisodios} completado.\")\n",
    "\n",
    "env.close() \n",
    "print(\"Entrenamiento finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
